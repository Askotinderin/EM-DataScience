\newpage
\section{Υλοποίηση}
Όπως έχουμε ήδη αναφέρει, για την υλοποίηση αλγορίθμων θα χρησιμοποιήσουμε την γλώσσα \textlatin{C++} και σκοπός μας είναι τα μοντέλα που θα δημιουργήσουμε να είναι εύχρηστα και επαναχρησιμοποιήσιμα και από
άλλους χρήστες. Θα χρειαστεί επομένως να δημιουργήσουμε κάποια εργαλεία με γενική χρήση. Στα πλαίσια αυτής της εργασίας υλοποιήθηκαν και εργαλεία τα οποία θα χρησιμοποιήσουμε αλλά δεν θα τα αναλύσουμε σε βάθος καθώς βγαίνουν εκτός του
θέματος της επιστήμης των δεδομένων. Αναφορικά αυτά τα εργαλεία είναι:
\begin{description}
    \item[\en{CSV Parser}] Υλοποιήσαμε κάποιες συναρτήσεις για να μπορούμε να εισάγουμε στο πρόγραμμα μας αρχεία \en{csv (comma seperated values)} που είναι μια πολύ συχνή μορφή δεδομένων που μάλιστα μπορεί να παραχθεί και από το \en{excel}
    \item[\en{Progress Bar}] Επειδή τα νευρωνικά δίκτυα και γενικότερα οι αλγόριθμοι μηχανικής μάθησης παίρνουν πολύ χρόνο υλοποιήσαμε μια γραμμή προόδου ώστε να μπορεί ο χρήστης ανά πάσα στιγμή να βλέπει σε ποιο στάδιο είναι το πρόγραμμα
    του και αν αυτό τρέχει σε φυσιολογικούς χρόνους.
    \item[\en{Tensor}] Ίσως από τις πιο σημαντικές υλοποιήσεις μας και καρδία πολλών αλγορίθμων μηχανικής μάθησης. Ο τένσορας είναι μια δομή όπως ένας πίνακας με τη διαφορά ότι μπορεί πολύ περισσότερες διαστάσεις. Στην υλοποίηση αυτή δημιουργήθηκε η δομή του τένσορα, όλες οι βασικές πράξεις και συναρτήσεις για αυτόν και επιπλέον έχει προστεθεί κώδικας για την παραλληλοποίηση του με αποτέλεσμα να είναι πολύ πιο γρήγορες οι πράξεις. Παρόλο που είναι ένα πολύ
    σημαντικό κομμάτι για τη μηχανική μάθηση δεν θα αναλυθεί παραπάνω καθώς η δομή του είναι μεγάλη και πολύπλοκη.
\end{description}

Η τελευταία υλοποίησή μας η οποία είναι η πιο σημαντική και την οποία θα αναλύσουμε στη συνέχεια είναι αυτή του νευρωνικού δικτύου. Σε αυτή την υλοποίηση δημιουργήσαμε τα βασικά επίπεδα που χρησιμοποιούνται στα νευρωνικά δίκτυα και διάφορες
συναρτήσεις υπολογισμού κόστους για διαφορετικούς τύπους προβλήματος. Επιπλέον δημιουργήσαμε τη δομή του δικτύου το οποίο μπορεί αποτελείται αποτελείται από πολλά επίπεδα και τη δομή του μοντέλου στην οποία μπορούμε να εισάγουμε ένα δίκτυο που θα φτιάξουμε, τα δεδομένα που θα εκπαιδεύσουμε και επιπλέον μπορούμε να δηλώσουμε και τις παραμέτρους εκπαίδευσης.
\subsection{Επίπεδα}
Τα δύο επίπεδα που επιλέξαμε να υλοποιήσουμε είναι το \en{dense layer (fully connected)} και το \en{activation layer}. Πριν όμως υλοποιήσουμε τα επίπεδα διεπαφή (\en{interface}). Η διεπαφή αυτή είναι το επίπεδο και ο κώδικας φαίνεται
παρακάτω.
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption=Layer Interface]
class Layer {
public:
    virtual ~Layer () {}
    virtual void forward (Tensor<double>* propagator) = 0;
    virtual void backwards (Tensor<double>* propagator) = 0;
    virtual void predict (Tensor<double>* propagator) = 0;
    virtual void fix (double learning_rate) = 0;
};
\end{lstlisting}
\end{otherlanguage}
Από τον παραπάνω κώδικά πρέπει να καταλάβουμε κάποια βασικά πράγματα. Αυτή η κλάση θα κληρονομηθεί από όλα τα επίπεδα που θα φτιάξουμε. Έχουμε ορίσει συγκεκριμένες συναρτήσεις και τις έχουμε αναθέσει σε μηδέν. Αυτό σημαίνει ότι κάθε κλάση
που θα την κληρονομήσει επιβάλλεται να υλοποιήσει αυτές τις συναρτήσεις (αλλιώς θα υπάρχει \en{error}). Αυτό θα μας βοηθήσει στην αντιμετώπιση προβλημάτων αλλά δίνει τη δυνατότητα σε έναν χρήστη να φτιάξει τα δικά του επίπεδα και να έχουν
απευθείας πλήρη συμβατότητα με το υπόλοιπο πρόγραμμα. Σε αυτό συμβάλει σημαντικά η λέξη \en{virtual} που βρίσκεται στην αρχή κάθε συνάρτησης αλλά θα εξηγήσουμε τη σημασία της παρακάτω.

Οι συναρτήσεις των οποίων επιβάλλεται η υλοποίηση είναι:
\begin{description}
    \item[\en{forward}] Δέχεται έναν τένσορα σαν  είσοδο και τον επεξεργάζεται (τον μετατρέπει ουσιαστικά στην έξοδο για να μπει σαν είσοδος στο επόμενο επίπεδο). Αυτή η συνάρτηση χρησιμοποιείται κατά την εκπαίδευση δώσουμε μια είσοδο στο
    δίκτυο μας και να υπολογίσει την έξοδο.
    \item[\en{backwords}] Ουσιαστικά έχει ίδια χρήση με την \en{forward} αλλά ενώ η \en{forward} διαδίδει την είσοδο στην έξοδο η \en{backwords} διαδίδει το σφάλμα προς την αντίθετη κατεύθυνση (\en{Back Propagation}).
    \item[\en{predict}] είναι ακριβώς ίδια στην υλοποίηση της με την \en{forward}, μόνο που παραλείπει κάποιες διεργασίες της που δεν είναι απαραίτητες. Χρησιμοποιείται στο στάδιο του \en{validation}.
    \item[\en{fix}] Είναι ο τρόπος με τον οποίο μαθαίνει το επίπεδο (και κατα συνέπεια και το δίκτυο) διορθώνοντας κάποιες παραμέτρους (βάρη) του επιπέδου.
\end{description}

Έχοντας κατανοήσει τα παραπάνω μπορούμε να φτιάξουμε το πρώτο μας επίπεδο (\en{dense}):
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Dense layer in hpp file]
class Dense : public Layer {
private:
    Tensor<double>* weights;
    Tensor<double>* biases;
    Tensor<double>* inputs;
    Tensor<double>* errors;
public:
    Dense (size_t input_size, size_t output_size);
    ~Dense ();
    void forward (Tensor<double>* propagator) override;
    void backwards (Tensor<double>* propagator) override;
    void predict (Tensor<double>* propagator) override;
    void fix (double learning_rate) override;
};
\end{lstlisting}
\end{otherlanguage}
Εδώ βλέπουμε τις επιπλέον παραμέτρους που χρειάζονται για αυτό το επίπεδο. Βλέπουμε τέσσερις τένσορες. Οι δύο από αυτους χρησιμοποιούνται για τον υπολογισμό της εξόδου (\en{weigts} και \en{biases}), ενώ οι άλλοι δύο χρησιμοποιούνται για
την αποθήκευση των εισόδων και των σφαλμάτων. Μπορούμε να δούμε την υλοποίηση των συναρτήσεων παρακάτω:
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Dense layer in cpp file]
Dense::Dense (size_t input_size, size_t output_size) {
    weights = new Tensor<double>(input_size, output_size);
    biases  = new Tensor<double>((size_t)1, output_size);
    inputs  = new Tensor<double>;
    errors  = new Tensor<double>;

    for(size_t i = 0; i < input_size; i++) {
            for( size_t j = 0; j < output_size; j++) {
                (*weights)(i, j) = rand()/(double)(RAND_MAX) - 0.5;
            }
        }
        for(size_t i = 0; i < output_size; i++) {
            (*biases)((size_t)0, i) = 0;
        }
}
Dense::~Dense () {
    delete weights;
    delete biases;
    delete inputs;
    delete errors;
}
void Dense::forward (Tensor<double>* propagator) {
    *inputs = *propagator;
    *propagator = mul(*propagator, *weights) + *biases;
}
void Dense::backwards (Tensor<double>* propagator) {
    *errors = *propagator;
    *propagator = mul(*propagator, ~(*weights));
}
void Dense::predict (Tensor<double>* propagator) {
    *propagator = mul(*propagator, *weights) + *biases;
}
void Dense::fix (double learning_rate) {
    *weights = *weights - mul(~(*inputs), *errors)*
        learning_rate/(double)errors->dims()[0];
    *biases = *biases - (*errors)[0]*
        learning_rate/(double)errors->dims()[0];
}
\end{lstlisting}
\end{otherlanguage}
Οι συναρτήσεις είναι οι εξής:
\begin{description}
    \item[\en{Desne (constructor)}] Δημιουργεί και αρχικοποιεί το επίπεδο. Συγκεκριμένα δέχεται το μέγεθος εισόδου και εξόδου και αρχικοποιεί με τα ανάλογα μεγέθη τους τους τένσορες για τα \en{weights} (αρχικοποιούνται τα στοιχεία του με
    τυχαίους αριθμους από -0.5 ως 0.5 )  και \en{biases} (αρχικοποιούνται σε μηδέν).
    \item[\en{\~\ Desne (destructor)}] Απελευθερώνει τη μνήμη που δέσμευαν οι τένσορες.
    \item[\en{forward}] Αρχικά σώσει την είσοδο (θα χρειαστεί σε επόμενο στάδιο). Έπειτα κάνει πολλαπλασιασμό πινάκων μεταξύ εισόδου και βαρών και μετά προσθέτει στο αποτέλεσμα τα \en{biases}.
    \item[\en{backwards}] Αρχικά σώζει το σφάλμα και έπειτα υπολογίζει το σφάλμα του προηγούμενου επιπέδου. Για να το κάνει αυτό εκτελεί πολλαπλασιασμό πινάκων μεταξύ των σφαλμάτων του και τον \textbf{αντίστροφο} των βαρών.
    \item[\en{predict}] Κάνει την ίδια δουλειά με το \en{forward} αλλά δεν χρειάζεται να αποθηκεύσει την είσοδο.
    \item[\en{fix}] Διορθώνει τα βάρη και τα \en{biases} με τον τρόπο που έχουμε ήδη δει.
\end{description}

Τώρα θα υλοποιήσουμε το \en{activation layer}.
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Activation layer in hpp file]
class Activation : public Layer {
private:
    Tensor<double>* inputs;
    std::function<void(Tensor<double>*)> func;
    std::function<void(Tensor<double>*)> der;
public:
    Activation (    size_t input_size,
                    std::function<void(Tensor<double>*)> func,
                    std::function<void(Tensor<double>*)> der);
    ~Activation ();
    void forward (Tensor<double>* propagator) override;
    void backwards (Tensor<double>* propagator) override;
    void predict (Tensor<double>* propagator) override;
    void fix (double learning_rate) override;
};
\end{lstlisting}
\end{otherlanguage}
Παρατηρούμε ότι αυτό το επίπεδο δεν έχει βάρη αλλά έχει δύο συναρτήσεις τις οποίες μπορούμε να δηλώσουμε εμείς κατα τη δημιουργία αυτού του επιπέδου. Συγκεκριμένα αρχικοποιούμε ένα τέτοιο επίπεδο δίνοντας το μέγεθος εισόδου, μια συνάρτηση
ενεργοποίησης και την παράγωγό της. Αυτή η υλοποίηση επιτρέπει σε οποιονδήποτε χρήστη να φτιάξει τις δικές του συναρτήσεις ενεργοποίησης και να μπορεί να τις χρησιμοποιήσει απευθείας σε συνδυασμό με τη βιβλιοθήκη μας. Η υλοποίηση των
συναρτήσεων για αυτό το επίπεδο είναι πολύ  πιο απλές.
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Activation layer in cpp file]
Activation::Activation (    size_t input_size,
                            std::function<void(Tensor<double>*)> func,
                            std::function<void(Tensor<double>*)> der) {
    inputs  = new Tensor<double>;
    this->func = func;
    this->der = der;
}
Activation::~Activation () {
    func = nullptr;
    der = nullptr;
    delete inputs;
}
void Activation::forward (Tensor<double>* propagator) {
    *inputs = *propagator;
    func(propagator);
}
void Activation::backwards (Tensor<double>* propagator) {
    der(inputs);
    *propagator = (*propagator) * (*inputs);
}
void Activation::predict (Tensor<double>* propagator) {
    func(propagator);
}
void Activation::fix (double learning_rate) {

}
\end{lstlisting}
\end{otherlanguage}
Όπως φαίνεται και παραπάνω η συνάρτηση \en{forward} αποθηκεύει την είσοδο και εφαρμόζει τη συνάρτηση ενεργοποίησης στην είσοδο, ενώ η συνάρτηση \en{backwards} πολλαπλασιάζει το σφάλμα με την παράγωγο την αποθηκευμένης εισόδου. Τα επίπεδα
ενεργοποίησης δεν μαθαίνουν κάτι αφού δεν έχουν βάρη άρα μπορούμε να αφήσουμε τη συνάρτηση κενή (αλλά πρέπει να την υλοποιήσουμε). Έχουμε επίσης υλοποιήσει τις πιο γνωστές συναρτήσεις ενεργοποίησης οι οποίες είναι:
\begin{enumerate}
    \item \en{sigmoid}
    \item \en{tanh}
    \item \en{relU}
    \item \en{linear}
    \item \en{softmax}
\end{enumerate}
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Activation functions]
void act::sigmoid (Tensor<double>* t) {
    *t = 1/(1 + exp(-(*t)));
}
void act::tanh (Tensor<double>* t) {
    *t = tanh(*t);
}
void act::relu (Tensor<double>* t) {
    *t = max(*t, 0);
}
void act::softmax (Tensor<double>* t) {
    Tensor<double> sums = exp(*t)[1];
    *t = exp(*t)/sums;
}
void act::linear (Tensor<double>* t) {

}
void der::sigmoid (Tensor<double>* t) {
    *t = exp(-(*t))/((1 + exp(-(*t)))*(1 + exp(-(*t))));
}
void der::tanh (Tensor<double>* t) {
    *t = 1 - tanh(*t)*tanh(*t);
}
void der::relu (Tensor<double>* t) {
    *t = (*t > 0);
}
void der::softmax (Tensor<double>* t) {

}
void der::linear (Tensor<double>* t) {

}
\end{lstlisting}
\end{otherlanguage}
Εδώ παρατηρούμε ότι οι παράγωγοι για των \en{linear} και \en{softmax} δεν έχουν υλοποιηθεί αλλά αυτό δεν μας επηρεάζει διότι αυτές οι ενεργοποιήσεις βρίσκονται μόνο στο επίπεδο εξόδου και αν κάνουμε καποιες μαθηματικές απλοποιήσεις
προκύπτει ότι δεν είναι απαραίτητη η οπισθοδρόμηση από αυτό το επίπεδο αλλά μπορούμε να περάσουμε απευθείας στο προηγούμενο.

\subsection{Δίκτυο}
Έχουμε λοιπόν υλοποιήσει τα δύο απαραίτητα επίπεδα για κάθε νευρωνικό δίκτυο μπορούμε να υλοποιήσουμε την κλάση του δικτύου.
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Network layer in hpp file]
class Network : public Layer {
private:
    std::vector<Layer*> layers;
public:
    Network ();
    ~Network ();
    void add (Layer* l);
    void forward (Tensor<double>* propagator) override;
    void backwards (Tensor<double>* propagator) override;
    void predict (Tensor<double>* propagator) override;
    void fix (double learning_rate) override;
};
\end{lstlisting}
\end{otherlanguage}
Βλέπουμε ότι το δίκτυο περιέχει μία λίστα από επίπεδα. Επιπλέον έχει όλες τις συναρτήσεις που είχαν και τα επίπεδα. Στη συνάρτηση \en{forward} διατρέχουμε για κάθε επίπεδο με τη σειρά και εκτελούμε τη συνάρτηση \en{forward} ενώ στη συνάρτηση
όπως κάνουμε και στις \en{predict} και \en{fix} ενώ στην \en{backwards} διατρέχουμε τα επίπεδα με την ανάποδη σειρά. Επιπλέον έχουμε και μια συνάρτηση για προσθέτουμε επίπεδα στο δίκτυο μας. Η υλοποίηση φαίνεται παρακάτω.
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Network layer in cpp file]
Network::Network () {
    layers = std::vector<Layer*>{};
}
Network::~Network () {
    for (auto layer : layers) delete layer;
    }
void Network::add (Layer* l) {
    layers.push_back(l);
}
void Network::forward (Tensor<double>* propagator) {
    for (auto layer : layers) layer->forward(propagator);
}
void Network::backwards (Tensor<double>* propagator) {
    for(size_t i = 1; i < layers.size(); i++)
        layers[layers.size() - i - 1]->backwards(propagator);
}
void Network::predict (Tensor<double>* propagator) {
    for (auto layer : layers) layer->predict(propagator);
}
void Network::fix (double learning_rate) {
    for (auto layer : layers) layer->fix(learning_rate);
}
\end{lstlisting}
\end{otherlanguage}
Έδω είναι σημαντικό να εξηγήσουμε τη λέξη \en{virtual} που χρησιμοποιήσαμε στη διεπαφή μας. Στην υλοποίηση του δικτύου μας έχουμε λίστα από επίπεδα. Επειδή όμως δεν η λίστα δεν περιέχει τα επίπεδα αλλά τους δείκτες σε αυτά μπορούμε να
αρχικοποιήσουμε ένα επίπεδο (που είναι μια αδεια κλάση) με τον \en{constructor} της κλάσης \en{Dense} γράφοντας \en{\lstinline[style=cppstyle]{Layer* l = new Dense(10, 5);}}. Το ερώτημα είναι όταν καλέσουμε τη συνάρτηση
\en{\lstinline[style=cppstyle]{l.forward(some tensor);}} θα καλέσουμε τη συνάρτηση \en{forward} της κλάσης \en{Layer} ή της \en{Dense}? Επειδή λοιπόν έχουμε προσθέσει τη λέξη \en{virtual} μπροστά από τις συναρτήσεις της διεπαφής θα
κληθεί η συνάρτηση του \en{Dense} ενώ διαφορετικά θα γινόταν το ανάποδο. Αυτό μας δίνει μεγάλη ευκολία στο να προσθέσουμε διαφορετικά επίπεδα στο δίκτυο με την ίδια συνάρτηση και να τα έχουμε όλα σε μία λίστα. Επιπλέον ένας χρήστης όπως
αναφέραμε μπορεί να φτιάξει δικά του επίπεδα και να τα χρησιμοποιήσει με απόλυτη συμβατότητα αρκεί να κληρονομούν την κλάση \en{Layer}.

\subsection{Μοντέλο}
Η τελευταία κλάση που θα δούμε είναι η κλάση \en{model}. Αυτή η κλάση περιέχει ένα δίκτυο και έχει μια συνάρτηση για να το εκπαιδεύει. Επιπλέον αυτή η κλάση κρατάει και τις παραμέτρους εκπαίδευσης του δικτύου που είναι:
\begin{description}
    \item[\en{batch\_size}] Πόσα δεδομένα στέλνουμε μαζί (\en{batch}) σε ένα \en{iteration} (δηλαδή πριν κάνουμε διόρθωση βαρών)
    \item[\en{iterations}] Πόσα \en{batches} δεδομένων θα στείλουμε σε ένα \en{epoch} (δηλαδή πριν κάνουμε \en{validation})
    \item[\en{epochs}] Πόσα \en{epochs} θα διαρκέσει η εκπαίδευση
    \item[\en{learning\_rate}] Πόσο γρήγορα μαθαίνει το δίκτυο
    \item[\en{validation\_split}] Το ποσοστό των δεδομένων που θα χρησιμοποιηθούν για εκπαίδευση (Τα υπόλοιπα πάνε στο \en{validation})
\end{description}

Επειδή η συνάρτηση που εκπαιδεύει το δίκτυο είναι πολύ μεγάλη δεν θα δείξουμε τον κώδικα αλλά οι παράμετροι εξηγούν τη λειτουργία αρκετά καλά και το μόνο που έχουμε να κάνουμε είναι να χρησιμοποιήσουμε επαναληπτικά, με βάση αυτές τις παραμέτρους, τις συναρτήσεις του δικτύου (\en{forward, backwards, predict, fix}).

Εδώ είναι επίσης το σημείο που πρέπει να φτιάξουμε τις συναρτήσεις που υπολογίζουν το σφάλμα για κάθε περίπτωση προβλήματος. Οι περιπτώσεις προβλήματος είναι:
\begin{enumerate}
    \item Παλινδρόμηση (\en{regression})
    \item Δυαδική ταξινόμηση (\en{binary classification})
    \item Κατηγορηματική ταξινόμηση (\en{categorical classification})
    \item Πολυταξική ταξινόμηση (\en{multy class classification})
\end{enumerate}
\begin{otherlanguage}{english}
\begin{lstlisting}[style=cppstyle,caption= Network layer in cpp file]
double err::regression (Tensor<double>* p, const Tensor<double>& e) {
    *p = *p - e;
    double cost = (abs(*p)[0][1]/(double)(p->dims()[0])).vec()[0];
    return cost;
}
double err::binary (Tensor<double>* p, const Tensor<double>& e) {
    Tensor<double> loss = -(e*log(*p) + ((double)1 - e)*log((double)1 - *p));
    *p = *p - e;
    double cost = (loss[0]/(double)(loss.dims()[0])).vec()[0];
    return cost;
}
double err::categorical (Tensor<double>* p, const Tensor<double>& e) {
    Tensor<double> loss = (-e*log(*p))[1];
    *p = *p - e;
    double cost = (loss[0]/(double)(loss.dims()[0])).vec()[0];
    return cost;
}
double err::multyclass (Tensor<double>* p, const Tensor<double>& e) {
    Tensor<double> loss = (-(e*log(*p) + ((double)1 - e)*log((double)1 - *p)))[1];
    *p = *p - e;
    double cost = (loss[0]/(double)(loss.dims()[0])).vec()[0];
    return cost;
}
\end{lstlisting}
\end{otherlanguage}
Έτσι μπορούμε να υπολογίσουμε το σφάλμα για κάθε είδος προβλήματος. Αυτές ήταν οι πιο σημαντικές υλοποιήσεις που έγιναν στα πλαίσια αυτής της εργασίας.